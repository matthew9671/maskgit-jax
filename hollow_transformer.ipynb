{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7263edf8-cb14-47e6-b51c-addad239d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Iterable, Optional, Text, Tuple, Union\n",
    "\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "from maskgit.nets.maskgit_transformer import Mlp, Bias, Embed#, MlmLayer\n",
    "\n",
    "LAYERNORM_EPSILON = 1e-12  # Layer norm from BERT\n",
    "\n",
    "InitializerType = Callable[[jnp.ndarray, Iterable[int], jnp.dtype], jnp.ndarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7deaa0fe-0d46-4471-aef2-d71962753c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import flax.linen as nn\n",
    "# import jax.numpy as jnp\n",
    "\n",
    "# # Example sequence of length 4\n",
    "# sequence = jnp.array([[1, 2, 3, 4]])\n",
    "\n",
    "# # Generate causal mask\n",
    "# causal_mask = nn.make_causal_mask(sequence)\n",
    "\n",
    "# print(causal_mask)\n",
    "\n",
    "# mask = jnp.tril(jnp.ones((4, 4)), k=-1)\n",
    "# print(mask)\n",
    "jnp.tril(jnp.ones((3, 3)), k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edda11ab-0bfa-4eeb-868a-6fa2260b40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(stddev: Union[float, jnp.ndarray], dtype=jnp.float32):\n",
    "\n",
    "  def init(key: jnp.ndarray, shape: Iterable[int], dtype: jnp.dtype = dtype):\n",
    "    return jax.random.truncated_normal(\n",
    "        key=key, lower=-2, upper=2, shape=shape, dtype=dtype) * stddev\n",
    "\n",
    "  return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "fb4f387a-e0ce-4ef3-b867-eeadadcfac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "  \"\"\"Attention layer that is part of each Transformer layer.\"\"\"\n",
    "  hidden_size: int\n",
    "  hidden_dropout_prob: float\n",
    "  num_attention_heads: int\n",
    "  attention_probs_dropout_prob: float\n",
    "  hidden_dropout_prob: float\n",
    "  initializer_fn: InitializerType\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, q: jnp.ndarray, kv: jnp.ndarray,\n",
    "               attention_mask: jnp.ndarray,\n",
    "               deterministic: bool) -> jnp.ndarray:\n",
    "\n",
    "    attention_output = nn.attention.MultiHeadAttention(\n",
    "        num_heads=self.num_attention_heads,\n",
    "        qkv_features=self.hidden_size,\n",
    "        out_features=None,\n",
    "        dropout_rate=self.attention_probs_dropout_prob,\n",
    "        deterministic=deterministic,\n",
    "        kernel_init=self.initializer_fn,\n",
    "        bias_init=jax.nn.initializers.zeros,\n",
    "        name='multi_head_attention',\n",
    "    )(q, kv, kv, mask=attention_mask)\n",
    "\n",
    "    attention_output = nn.Dropout(rate=self.hidden_dropout_prob)(\n",
    "        attention_output, deterministic=deterministic)\n",
    "    attention_output = nn.LayerNorm(\n",
    "        epsilon=LAYERNORM_EPSILON, name='attention_output_ln')(\n",
    "            attention_output + q)\n",
    "\n",
    "    return attention_output\n",
    "\n",
    "class GenericTransformerLayer(nn.Module):\n",
    "  \"\"\"A single Transformer layer.\"\"\"\n",
    "  intermediate_size: int\n",
    "  hidden_size: int\n",
    "  hidden_dropout_prob: float\n",
    "  num_attention_heads: int\n",
    "  attention_probs_dropout_prob: float\n",
    "  initializer_fn: InitializerType\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, q: jnp.ndarray, kv: jnp.ndarray, mask: jnp.ndarray,\n",
    "               deterministic: bool) -> jnp.ndarray:\n",
    "      \n",
    "    attention_output = CausalAttention(\n",
    "        hidden_size=self.hidden_size,\n",
    "        hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "        num_attention_heads=self.num_attention_heads,\n",
    "        attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "        initializer_fn=self.initializer_fn)(\n",
    "            q=q, kv=kv, attention_mask=mask,\n",
    "            deterministic=deterministic)\n",
    "      \n",
    "    layer_output = Mlp(\n",
    "        hidden_size=self.hidden_size,\n",
    "        hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "        intermediate_size=self.intermediate_size,\n",
    "        initializer_fn=self.initializer_fn)(\n",
    "            attention_output=attention_output, deterministic=deterministic)\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "class HollowTransformer(nn.Module):\n",
    "  \"\"\"Hollow transformer modified from BERT.\"\"\"\n",
    "  vocab_size: int\n",
    "  hidden_size: int = 768\n",
    "  num_hidden_layers: int = 12\n",
    "  num_attention_heads: int = 12\n",
    "  intermediate_size: int = 3072\n",
    "  hidden_dropout_prob: float = 0.1\n",
    "  attention_probs_dropout_prob: float = 0.1\n",
    "  max_position_embeddings: int = 256\n",
    "  initializer_range: float = 0.02\n",
    "  num_layers_per_mixed: int = 4 \n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               input_ids: jnp.ndarray,\n",
    "               deterministic: bool = True) -> Dict[Text, jnp.ndarray]:\n",
    "\n",
    "    input_ids = input_ids.astype('int32')\n",
    "    x = Embed(\n",
    "        embedding_size=self.hidden_size,\n",
    "        hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "        vocab_size=self.vocab_size,\n",
    "        max_position_embeddings=self.max_position_embeddings,\n",
    "        initializer_fn=truncated_normal(self.initializer_range))(\n",
    "            input_ids=input_ids, deterministic=deterministic)\n",
    "    \n",
    "    B, L, K = x.shape\n",
    "      \n",
    "    forward_mask = jnp.tile(jnp.tril(jnp.ones((L, L)))[None], (B, 1))\n",
    "    backward_mask = jnp.tile(jnp.triu(jnp.ones((L, L)))[None], (B, 1))\n",
    "    mixing_mask = jnp.concatenate([forward_mask, backward_mask], axis=2)      \n",
    "\n",
    "    # Causal attention doesn't like zero padding\n",
    "    pad = jnp.ones((B, 1, K))\n",
    "    xf = jnp.concatenate([pad, x[:,:-1]], axis=1)\n",
    "    xb = jnp.concatenate([x[:,1:], pad], axis=1)\n",
    "    xm = None\n",
    "      \n",
    "    for i in range(self.num_hidden_layers):\n",
    "      f_layer = GenericTransformerLayer(\n",
    "          intermediate_size=self.intermediate_size,\n",
    "          hidden_size=self.hidden_size,\n",
    "          hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "          num_attention_heads=self.num_attention_heads,\n",
    "          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "          initializer_fn=truncated_normal(self.initializer_range))\n",
    "      b_layer = GenericTransformerLayer(\n",
    "          intermediate_size=self.intermediate_size,\n",
    "          hidden_size=self.hidden_size,\n",
    "          hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "          num_attention_heads=self.num_attention_heads,\n",
    "          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "          initializer_fn=truncated_normal(self.initializer_range))\n",
    "      xf = f_layer(q=xf, kv=xf, mask=forward_mask, deterministic=deterministic)\n",
    "      xb = b_layer(q=xb, kv=xb, mask=backward_mask, deterministic=deterministic)\n",
    "\n",
    "      if (i + 1) % self.num_layers_per_mixed == 0:\n",
    "        if xm is None:\n",
    "          xm = jnp.concatenate([xf, xb], axis=2)\n",
    "        xfb = jnp.concatenate([xf, xb], axis=1)\n",
    "        m_layer = GenericTransformerLayer(\n",
    "          intermediate_size=self.intermediate_size,\n",
    "          hidden_size=self.hidden_size * 2, # since we're combining the streams\n",
    "          hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "          num_attention_heads=self.num_attention_heads,\n",
    "          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "          initializer_fn=truncated_normal(self.initializer_range))\n",
    "        xm = m_layer(q=xm, kv=xfb, mask=mixing_mask, deterministic=deterministic)\n",
    "\n",
    "    layer_output = xm\n",
    "      \n",
    "    word_embedding_matrix = self.variables['params']['Embed_0'][\n",
    "        'word_embeddings']['embedding']\n",
    "    logits = MlmLayer(\n",
    "        hidden_size=self.hidden_size,\n",
    "        initializer_fn=truncated_normal(self.initializer_range))(\n",
    "            last_layer=layer_output, embeddings=word_embedding_matrix)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ddc22a0c-4405-4b33-a50d-fa70fddea53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "model = HollowTransformer(\n",
    "  vocab_size = 4,\n",
    "  hidden_size = k,\n",
    "  num_hidden_layers = 3,\n",
    "  num_attention_heads = 2,\n",
    "  intermediate_size = 10,\n",
    "  hidden_dropout_prob = 0.0,\n",
    "  attention_probs_dropout_prob = 0.0,\n",
    "  max_position_embeddings = 10,\n",
    "  initializer_range = 1.,#0.02\n",
    "  num_layers_per_mixed = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "5d2001f7-e964-4f55-abc9-1b9de28b58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "5e156ee5-b057-47e9-b5ac-071fe1a9be97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[ 0.68474483, -3.8607855 ,  4.3750286 , -0.04215145],\n",
       "        [ 0.2565856 , -3.9484396 ,  4.4713316 ,  0.37558365],\n",
       "        [-0.09743834, -3.1910896 , -0.19396186, -1.9386406 ],\n",
       "        [-0.39917445, -4.6603518 ,  2.432427  ,  0.95605516],\n",
       "        [ 0.42935085, -4.712538  ,  3.9293957 ,  1.0363998 ],\n",
       "        [ 1.1949406 , -3.6340714 ,  3.7268753 ,  1.0710907 ],\n",
       "        [ 0.47236824, -4.36226   ,  4.489441  ,  0.17659187],\n",
       "        [ 0.38882256, -4.580062  ,  4.3808365 , -0.0818696 ],\n",
       "        [ 0.00630283, -4.956742  ,  3.8312683 , -0.01543999],\n",
       "        [-0.7918043 , -4.4137115 ,  4.0304804 ,  0.31813812]]],      dtype=float32)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key, _ = jr.split(key)\n",
    "input_sequence = jr.normal(shape=(1, 10), key=key)\n",
    "params = model.init(key, input_sequence, True)\n",
    "model.apply(params, input_sequence, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "d72aae9e-ef77-4c29-945a-e938ac8de451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the \"hollowness\" we need to replace the gradient with finite differences\n",
    "# since the embedding step is not differentiable\n",
    "\n",
    "# l = 7\n",
    "# grad_fn = jax.grad(lambda x: jnp.sum(model.apply(params, x, True)[0, l, 0]))\n",
    "# grad_fn(input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249b324-461d-48f0-839d-6d307f516bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (maskgit_jax)",
   "language": "python",
   "name": "maskgit_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
