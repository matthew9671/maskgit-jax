{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7263edf8-cb14-47e6-b51c-addad239d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Iterable, Optional, Text, Tuple, Union\n",
    "\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "\n",
    "from maskgit.nets.maskgit_transformer import Mlp, Bias, Embed, MlmLayer\n",
    "\n",
    "LAYERNORM_EPSILON = 1e-12  # Layer norm from BERT\n",
    "\n",
    "InitializerType = Callable[[jnp.ndarray, Iterable[int], jnp.dtype], jnp.ndarray]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edda11ab-0bfa-4eeb-868a-6fa2260b40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(stddev: Union[float, jnp.ndarray], dtype=jnp.float32):\n",
    "\n",
    "  def init(key: jnp.ndarray, shape: Iterable[int], dtype: jnp.dtype = dtype):\n",
    "    return jax.random.truncated_normal(\n",
    "        key=key, lower=-2, upper=2, shape=shape, dtype=dtype) * stddev\n",
    "\n",
    "  return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb4f387a-e0ce-4ef3-b867-eeadadcfac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "  \"\"\"Attention layer that is part of each Transformer layer.\"\"\"\n",
    "  hidden_size: int\n",
    "  hidden_dropout_prob: float\n",
    "  num_attention_heads: int\n",
    "  attention_probs_dropout_prob: float\n",
    "  hidden_dropout_prob: float\n",
    "  initializer_fn: InitializerType\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, q: jnp.ndarray, kv: jnp.ndarray,\n",
    "               attention_mask: jnp.ndarray,\n",
    "               deterministic: bool) -> jnp.ndarray:\n",
    "\n",
    "    attention_output = nn.attention.MultiHeadAttention(\n",
    "        num_heads=self.num_attention_heads,\n",
    "        qkv_features=self.hidden_size,\n",
    "        out_features=None,\n",
    "        dropout_rate=self.attention_probs_dropout_prob,\n",
    "        deterministic=deterministic,\n",
    "        kernel_init=self.initializer_fn,\n",
    "        bias_init=jax.nn.initializers.zeros,\n",
    "        name='multi_head_attention',\n",
    "    )(q, kv, kv, mask=attention_mask)\n",
    "\n",
    "    attention_output = nn.Dropout(rate=self.hidden_dropout_prob)(\n",
    "        attention_output, deterministic=deterministic)\n",
    "    attention_output = nn.LayerNorm(\n",
    "        epsilon=LAYERNORM_EPSILON, name='attention_output_ln')(\n",
    "            attention_output + q)\n",
    "\n",
    "    return attention_output\n",
    "\n",
    "class GenericTransformerLayer(nn.Module):\n",
    "  \"\"\"A single Transformer layer.\"\"\"\n",
    "  intermediate_size: int\n",
    "  hidden_size: int\n",
    "  hidden_dropout_prob: float\n",
    "  num_attention_heads: int\n",
    "  attention_probs_dropout_prob: float\n",
    "  initializer_fn: InitializerType\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, q: jnp.ndarray, kv: jnp.ndarray, mask: jnp.ndarray,\n",
    "               deterministic: bool) -> jnp.ndarray:\n",
    "      \n",
    "    attention_output = CausalAttention(\n",
    "        hidden_size=self.hidden_size,\n",
    "        hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "        num_attention_heads=self.num_attention_heads,\n",
    "        attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "        initializer_fn=self.initializer_fn)(\n",
    "            q=q, kv=kv, attention_mask=mask,\n",
    "            deterministic=deterministic)\n",
    "      \n",
    "    layer_output = Mlp(\n",
    "        hidden_size=self.hidden_size,\n",
    "        hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "        intermediate_size=self.intermediate_size,\n",
    "        initializer_fn=self.initializer_fn)(\n",
    "            attention_output=attention_output, deterministic=deterministic)\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "class HollowTransformer(nn.Module):\n",
    "  \"\"\"Hollow transformer modified from BERT.\"\"\"\n",
    "  vocab_size: int\n",
    "  hidden_size: int = 768\n",
    "  num_hidden_layers: int = 12\n",
    "  num_attention_heads: int = 12\n",
    "  intermediate_size: int = 3072\n",
    "  hidden_dropout_prob: float = 0.1\n",
    "  attention_probs_dropout_prob: float = 0.1\n",
    "  max_position_embeddings: int = 256\n",
    "  initializer_range: float = 0.02\n",
    "  num_layers_per_mixed: int = 4 \n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self,\n",
    "               input_ids: jnp.ndarray,\n",
    "               t: float,\n",
    "               deterministic: bool = True) -> Dict[Text, jnp.ndarray]:\n",
    "\n",
    "    B, L = input_ids.shape\n",
    "    # Causal attention doesn't like zero padding\n",
    "    pad = jnp.zeros((B, 1))\n",
    "    input_ids = jnp.concatenate([pad, input_ids, pad], axis=1)\n",
    "\n",
    "    input_ids = input_ids.astype('int32')\n",
    "    x = Embed(\n",
    "        embedding_size=self.hidden_size,\n",
    "        hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "        vocab_size=self.vocab_size,\n",
    "        max_position_embeddings=self.max_position_embeddings + 2, # Including the padded values\n",
    "        initializer_fn=truncated_normal(self.initializer_range))(\n",
    "            input_ids=input_ids, deterministic=deterministic)\n",
    "    \n",
    "    # B, L, K = x.shape\n",
    "    H = self.num_attention_heads\n",
    "      \n",
    "    forward_mask = jnp.tile(jnp.tril(jnp.ones((L, L)))[None, None], (B, H, 1, 1))\n",
    "    backward_mask = jnp.tile(jnp.triu(jnp.ones((L, L)))[None, None], (B, H, 1, 1))\n",
    "    mixing_mask = jnp.concatenate([forward_mask, backward_mask], axis=-1)   \n",
    "\n",
    "    # Causal attention doesn't like zero padding\n",
    "    # pad = jnp.ones((B, 1, K))\n",
    "    xf = x[:,:-2] #jnp.concatenate([pad, x[:,:-1]], axis=1)\n",
    "    xb = x[:,2:] #jnp.concatenate([x[:,1:], pad], axis=1)\n",
    "    xm = None\n",
    "      \n",
    "    for i in range(self.num_hidden_layers):\n",
    "      f_layer = GenericTransformerLayer(\n",
    "          intermediate_size=self.intermediate_size,\n",
    "          hidden_size=self.hidden_size,\n",
    "          hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "          num_attention_heads=self.num_attention_heads,\n",
    "          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "          initializer_fn=truncated_normal(self.initializer_range))\n",
    "      b_layer = GenericTransformerLayer(\n",
    "          intermediate_size=self.intermediate_size,\n",
    "          hidden_size=self.hidden_size,\n",
    "          hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "          num_attention_heads=self.num_attention_heads,\n",
    "          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "          initializer_fn=truncated_normal(self.initializer_range))\n",
    "      xf = f_layer(q=xf, kv=xf, mask=forward_mask, deterministic=deterministic)\n",
    "      xb = b_layer(q=xb, kv=xb, mask=backward_mask, deterministic=deterministic)\n",
    "\n",
    "      if (i + 1) % self.num_layers_per_mixed == 0:\n",
    "        if xm is None:\n",
    "          xm = jnp.concatenate([xf, xb], axis=2)\n",
    "        xfb = jnp.concatenate([xf, xb], axis=1)\n",
    "        m_layer = GenericTransformerLayer(\n",
    "          intermediate_size=self.intermediate_size,\n",
    "          hidden_size=self.hidden_size * 2, # since we're combining the streams\n",
    "          hidden_dropout_prob=self.hidden_dropout_prob,\n",
    "          num_attention_heads=self.num_attention_heads,\n",
    "          attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n",
    "          initializer_fn=truncated_normal(self.initializer_range))\n",
    "        xm = m_layer(q=xm, kv=xfb, mask=mixing_mask, deterministic=deterministic)\n",
    "\n",
    "    layer_output = xm\n",
    "      \n",
    "    word_embedding_matrix = self.variables['params']['Embed_0'][\n",
    "        'word_embeddings']['embedding']\n",
    "    logits = MlmLayer(\n",
    "        hidden_size=self.hidden_size,\n",
    "        initializer_fn=truncated_normal(self.initializer_range))(\n",
    "            last_layer=layer_output, embeddings=word_embedding_matrix)\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddc22a0c-4405-4b33-a50d-fa70fddea53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "vocab_size = 1024\n",
    "seq_len = 10\n",
    "model = HollowTransformer(\n",
    "  vocab_size = vocab_size + 1,\n",
    "  hidden_size = k,\n",
    "  num_hidden_layers = 6,\n",
    "  num_attention_heads = 2,\n",
    "  intermediate_size = 10,\n",
    "  hidden_dropout_prob = 0.0,\n",
    "  attention_probs_dropout_prob = 0.0,\n",
    "  max_position_embeddings = seq_len + 1,\n",
    "  initializer_range = 1.,#0.02\n",
    "  num_layers_per_mixed = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d2001f7-e964-4f55-abc9-1b9de28b58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "jax.config.update(\"jax_debug_nans\", False)\n",
    "jax.config.update(\"jax_debug_infs\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5e156ee5-b057-47e9-b5ac-071fe1a9be97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.18707103  -1.7576226    1.7236228    1.4114002    0.58498496\n",
      "   0.          -3.6051326   -0.6251073  -27.829098   -23.07043   ]\n"
     ]
    }
   ],
   "source": [
    "key, _ = jr.split(key)\n",
    "input_sequence = jr.categorical(logits=jnp.ones((vocab_size,)), shape=(1, seq_len), key=key)\n",
    "params = model.init(key, input_sequence, 0, True)\n",
    "\n",
    "# def loss_fn(params):\n",
    "#     eps = 1e-6\n",
    "#     out = model.apply(params, input_sequence[0], 0, True)\n",
    "#     out = out[0]\n",
    "#     p = jax.nn.softmax(out, axis=-1)\n",
    "#     x0_one_hot = jax.nn.one_hot(input_sequence, vocab_size + 1)\n",
    "#     logits = jnp.log(p + eps)\n",
    "#     return -jnp.mean(x0_one_hot * logits)\n",
    "    \n",
    "# grad_fn = jax.grad(loss_fn)\n",
    "# grads = grad_fn(params)\n",
    "\n",
    "l = 5\n",
    "\n",
    "out1 = model.apply(params, input_sequence, 0, True)\n",
    "input_sequence = input_sequence.at[0, l].set((input_sequence[0, l] - 1) % vocab_size)\n",
    "out2 = model.apply(params, input_sequence, 0, True)\n",
    "print(jnp.sum(out1[0] - out2[0], axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (maskgit_jax)",
   "language": "python",
   "name": "maskgit_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
